Рекомендательная система для новостного портала.

Рекомендательная система - это вещь с которой сталкивался каждый житель интернета в той или иной мере, но как она работает? В месте с ней часто употрябляют магические слова: 'машинное обучение', 'анализ данных', но что это такое?
Всё началось с того, что нам предложили заняться разработкой рекомендательной системы для портала 'Дождя'. 

Зачем нужна рекомендательная система?
Рекомендательная система - это система, которая упрощает жизнь пользователя. С помощью неё ваш путь по сайту меняется - вам не нужно постоянно возвращаться на главную страницу и искать релевантные новости, эти новости будут отображаться конкретно для вас в той новости, которую вы читаете.


С чего начать

Основой рекомендательной системы являлась бинарная классификиция. Бинарная классификация - это одна из самых распространённых проблем машинного обучения. К ней можно свести множество очень интересных и важных проблем. Как следует из названия бинарная классификация - это разделение объектов на 2 класса. Но что же можно разделять в новостях?

Данные

В данных, которые мы содержалась информация о 'перемещении' пользователя. Когда и куда он заходил на сайте, какие новости читал. С помощью этих данных мы выстроили сеанс пользователя. В сеансе пользователя содержалось n статей, которые он посещал в один периуд времени. Мы взяли три статьи из сеанса пользователя и сгенерировали для них 'features' - фичи для бинарного классификатора (об этом ниже). В бинарный классификатор попадают 4 статьи - 3, которые мы взяли из сеанса пользователя, они никогда не меняются, а 4 - новость, которая была опубликована в ближайшее 5 часов (об этом тоже ниже). Далее с помощью бинарного классификатора мы предсказываем на сколько подходит статья пользователю. 1 - подходит идеально, 0 - не подходит вообще.

В качестве обучения мы использовали хорошие сеансы пользователей (1 кластер) и сгенерироваль рандомные сеансы пользователей, которые априори были плохими (2 кластер).

И так, магическая константа 5 часов - она была получина отнють не рандомом, а с помощью того самого анализа данных. Проанализировав все сеансы пользователей мы получили график зависимости количества просмотров новости от времени существования новости - и вот, что у нас получилось:

Features

Фичи - это преобразование данных в машиночитаймый вид, например мы не можем просто так загнать текст в какую-либо модель, нам нужно либо преобразовать этот текст, либо вытащить из него фичи, например, сколько раз употреблялось в тексте 'пакет Яровой'. В нашей задачи у нас было 163 фичи:
Первые 160 - это фичи Topic Modeling-а (отношение к каждой из 40 тем для всех 4-х новостей)
3 фичи - это косинусная мера зависимости каждой из трёх с четвёртой.

А теперь подробнее о фичах:

Topic Modeling это одна из проблем машинного обучения без учителя - мы не знаем к каким темам относятся документы. Грубо говоря: у нас есть 10k документов лежащих в одном месте и нам нужно разбить их на темы. В Topic Modeling есть очень много разных решений, но перепробовав основную часть мы выбрали наилучшее, что подходило нам. Самым распространённым решением Topic Modeling-a это латентное размещение Дирихле на борте у этой штуки есть много занимательной математики, которую я объяснять не ручаюсь, но оставляю [ссылку](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) на неё. Если в кратце, то LDA видит документ, как набор из тематик и для каждой тематики выделяет набор слов, который относит новость к этой тематики. На наших данных LDA сработал хуже, чем NMF (неотрицательное разложение матрицы) очень хорошее объяснение этой модели можно найти [тут](http://derekgreene.com/nmf-topic/) там же есть demo на python, кому не лень. И так лучшим для нашей задачи, после долгих сравнений разных алгоритмов, оказался NMF. С помощью него мы выделили 40 тем, а дальше считали похожесть статьи к каждой из тем => 40*4 = 160 фичей.

Косинусная мера.

[Сева опиши]

Решение готово, но как проверить?
Для проверки решений используются метрики ранжирования, которые отлично описаны в [данной](https://habrahabr.ru/company/econtenta/blog/303458/) статье. Мы использовали Mean Average Precision at 10 (MAP@10).

Что делать после улучшения метрики?
