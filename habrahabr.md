-----------------------------------------------------------------------------------------------------------------------
![](https://hsto.org/getpro/habr/post_images/f98/aaf/d0c/f98aafd0c93f02ef2c367b4e6c81bf85.png)

Одно из основных особенностей школы GoTo - это то, что она направлена на практические применения знаний и каждый ученик (или сразу несколько) в конце смены представляет свой проект, который он разработал за это время.

Мы попросили реальную задачу от одной из компаний партнёров (E-Contenta) - создать рекомендательную систему для новостного портала телеканала. Это было довольно интересной задачей - помимо реальной необходимости такого рода разработок, была еще некоторая уникальность и нетипичность задачи - методы рекомендации новостей в большинстве отличаются от методов рекомендации, допустим, фильмов. После получения задачи мы начали её решать.

-----------------------------------------------------------------------------------------------------------------------

Рекомендательная система - это программа, позволяющая предсказать наиболее интересные объекты (например, книги, фильмы или статьи), уже имея какие-то данные о текущем состоянии. Состоянием могут быть уже понравившееся пользователю объекты, или данные, которые мы знаем о клиенте (например, его музыкальные предпочтения). Такие данные можно без труда получить с помощью логирования действий пользователя на сайте, собиранием внешней информации о пользователе или об объекте.

Основная задача, которую решает рекомендательная система  - это повышение удобства использования продукта конечному пользователю. Мы должны предсказать, что пользователю понравится с наибольшей вероятностью и тогда он будет избавлен от необходимости искать это самому, что позволит нам удержать пользователя на нашем ресурсе. Благодаря качественным рекомендательным системам выросли такие цифровые гиганты как Netflix, Spotify и многие другие стартапы.

Среди рекомендательных систем выделяются три основных типа: коллаборативная фильтрация, контетная и гибридная.

Коллаборативная фильтрация - наверное, наиболее популярная модель для рекомендации объектов. Её основная идея заключается в том, что если объекты смотрят почти одинаковые пользователи, то эти объекты стоит рекомендовать этим пользователям.

![](https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif)

Например, если некой Алисе нравится сериалы "Друзья" и "Теория Большого Взрыва", а некому Бобу нравятся "Бруклин 9-9" и "Друзья", то можно порекомендовать "Бруклин 9-9" Алисе и "Теорию Большого Взрыва" Бобу.

В коллаборативной фильтрации выделяется два основных подхода:

1. Корреляционные модели - основная идея таких моделий основана на хранении матрицы пользователей/объектов.
2. Латентные модели - модели, которые позволяют не держать матрицу пользователей/объектов, а строятся на основе 'профилей' пользователей и объектов. Профиль - это вектор скрытых характеристик.

Следующий способ построения рекомендательной модели - контетные рекомендации. Это значит, что наша модель будет зависеть от содержимого объектов. Например, можно оценивать похожесть текстов новостей (о том, как именно этот делать - чуть позже) или к фильму "Титаник" рекомендовать другие фильмы Кэмерона. Главная идея этого метода заключается в том, что мы пытаемся достать как можно большую информацию об объекте, который мы хотим порекомендовать, и используем эту информацию для поиска таких же объектов, после чего мы просто рекомендуем похожие объекты.

В нашей задаче по рекомендации новостей мы решили использовать гибридную модель - она комбинирует результаты предыдущих вариантов и возвращает взвешенный результат.

Наша гибридная модель на основе признаков, которые мы вытащили для объекта и для пользователя возвращает вероятность того, что пользователь прочитает эту статью (кликнет на неё). После нескольких тестов, в качестве алгоритма машинного обучения мы решили использовать Random Forest, но это не так принципиально.

Во всех задачах машинного обучения важно посмотреть на распределение данных, построить зависимости, которые помогут тебе обучить модель, без этого хорошего результата получить вы не сможете. Например, в нашей задаче прежде чем написать модель мы построили вот такой график:

![](https://github.com/xenx/recommendation_system/blob/master/graph.png)

Суть гибридной модели в том, что в качестве фичей мы предоставляем ей какие-то значения, полученные с помощью коллаборативной фильтрации и какие-то значения, полученные с помощью контентной модели.

Начнем с коллаборативной фильтрации. Давайте будем вычислять похожесть новостей по пользователям, которые смотрели эту статью. Для этого часто используется косинусная мера - косинус угла между двумя векторами, в данном случае - просмотрами пользователей.

![косинусная мера](https://wikimedia.org/api/rest_v1/media/math/render/svg/2a8c50526e2cc7aa837477be87eff1ea703f9dec)

Так как мы хотим, чтобы при рекомендациях учитывалась не только одна новость, возьмем три последние статьи и одну возможную (вероятность перехода на которую мы оцениваем), после чего посчитаем косинусную похожесть от каждой из прочитанных новостей к возможной. Таким образом у нас получится 3 фичи.

Теперь есть более сложная задача - оценивать похожесть новостей по их содержимому. Мы отмели самые простые варианты вроде поиска ключевых слов и подсчета их пересечений из-за маленькой эффективности.

В области построения контентных моделей отдельно стоит тематическое моделирование - способ разбиения документов по темам без учителя. Для этого существует несколько алгоритмов, в нашей системе мы использовали NMF - разложение неотрицательных матриц, она показала себя лучше чем LDA - латентное размещение Дирихле. Перед тем, как использовать NMF, нам необходимо построить матрицу, которую мы будем раскладывать, для этого использовали TF-IDF.

TF-IDF состоит из двух частей:

TF (term frequency — частота слова) - это отношение количеству данного слова в документе к количеству слов в данном документе. С помощью term frequency можно оценить важность слова в документе. Заметим, что частота так называемых стоп слов - союзы и союзные слова, местоимения, предлоги, частицы и т.д.) будет больше чем других, поэтому мы очистили все новости от них.

![tf](https://wikimedia.org/api/rest_v1/media/math/render/svg/92a19022b85d3796b7e6237ea6829cb550ef17ff)

IDF (inverse document frequency — обратная частота документа) - инверсия частоты, с которой некоторое слово встречается в документах коллекции. С помощью IDF мы можем выбросить не только стоп слова, но и понижать важность для часто употребляемых существительных.

![idf](https://wikimedia.org/api/rest_v1/media/math/render/svg/1c1f3347300bd19654bedfaef73861cf75ac5e65)

Поскольку нам важно знать и важность слова в тексте и уникальность мы перемножаем TF-IDF и получаем:

![TF-IDF](https://wikimedia.org/api/rest_v1/media/math/render/svg/fa3cf0b54c09151473641f8364c2da3480cc98f1)

С помощью TF-IDF мы создаём матрицу документы на слова, на пересечении документа и слова мы ставим значение полученное из TF-IDF:

![matrix](http://www.jiem.org/index.php/jiem/article/viewFile/293/252/2402)

Получив терм-документную матрицу мы раскладываем её по сингулярным значениям с помощью NMF. После разложения матрицы мы можем представить любой документ и термин в виде вектора в пространстве, близость между любой комбинацией терминов и/или документов легко вычисляется с помощью скалярного произведения векторов. После всего этого мы получили вполне осмысленные темы, например, в один кластер попали статьи про Савченко, в другой - про законы, которые предлагает Госдума.

Теперь каждую новость можно рассматривать как точку в 40-мерном пространстве (всего получилось 40 кластеров), где каждое измерение - это степень принадлежности к какому-то кластеру.
В качестве фичей будем использовать 40 значений для каждой из четырех новостей. Кроме того, можно заняться feature engineering и добавить разность каждой прочитанной статьи с возможной, но этого мы не сделали.

В результате, у нас получилась модель, дающая вполне осмысленные результаты. Например, новостям о возможном назначении кого-нибудь на должность советника Президента по интернету, рекомендуются статьи уже с конкретными именами.

Тем не менее, таких результатов недостаточно и нужно посчитать какую-нибудь метрику качества рекомендаций. Такие метрики очень связаны с оценкой качества ранжирования, поэтому будем использовать MAP@10. Результат - примерно 0.75, что довольно хорошо.

После создания модели, мы собрали веб-приложение с помощью Flask. Его исходники доступны на [Github](https://github.com/xenx/recommendation_system).

За время работы на проектом, мы заметили важную особенность подобных рекомендательных систем: они очень часто загоняют пользователей в ловушку статей на одну и ту же тему, поэтому стоит предусмотреть включения случайных статей, чтобы попробовать найти другие тему, интересные пользователю.
